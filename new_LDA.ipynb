{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1c9d20-6779-4721-9ffd-1f25c589feac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# import pyro\n",
    "# import pyro.infer\n",
    "# import pyro.optim\n",
    "# import pyro.distributions as dist\n",
    "import torch.distributions.constraints as constraints\n",
    "# import pyro.poutine as poutine\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# pyro.set_rng_seed(1)\n",
    "# pyro.clear_param_store()\n",
    "\n",
    "\n",
    "#############################\n",
    "######## TODO ###############\n",
    "####### ADD LEMMATZATIONN ###\n",
    "#############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a185dc4-2d33-485e-8f7d-76645536ac80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.2.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (1.26.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.9.0,>=2023.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (0.35.3)\n",
      "Requirement already satisfied: packaging in /Users/nedamohseni/Library/Python/3.12/lib/python/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.0)\n",
      "Requirement already satisfied: anyio in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.5)\n",
      "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.6)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.1.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5f17a1ea-c958-40ab-8e98-810cf911ff17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "ds = load_dataset(\"armanc/pubmed-rct20k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b14e571d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs after cleaning: 600 | Vocab size: 1911 | min_df=5, max_df=270\n",
      "#docs=600, vocab size=1911, total tokens=46378\n",
      "Sample tokens: ['counselling', 'older', 'people', 'chronic', 'condition', 'live', 'home', 'useful', 'support', 'paper', 'development', 'testing', 'instrument', 'counselling', 'questionnaire', 'developed', 'tested', 'people', 'older', 'component', 'conducted', 'revealed', 'four', 'factor', 'model']\n",
      "Sample tokens: ['pharmacokinetics', 'doses', 'investigated', 'application', 'two-way', 'cross-over', 'conducted', 'seven', 'healthy', 'postmenopausal', 'women', 'age', 'allocated', 'applied', 'upper', 'serum', 'total', 'free', 'sex', 'concentrations', 'serum', 'pharmacokinetic', 'parameters', 'auc', 'cmax']\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# PubMed LDA Preprocessing for ~600 docs (Gibbs unchanged)\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from datasets import load_dataset, logging as ds_logging\n",
    "\n",
    "# ---- Config for ~600 abstracts ----\n",
    "RANDOM_STATE     = 42\n",
    "SAMPLE_N         = 600\n",
    "MIN_TOKENS_DOC   = 20          # drop ultra-short docs after cleaning\n",
    "MIN_DF_ABS       = 5           # absolute min_df for ~600 docs\n",
    "MAX_DF_FRAC      = 0.45        # prune top ~45% most ubiquitous tokens\n",
    "PHRASE_MIN_COUNT = 5           # tuned for ~600 docs\n",
    "PHRASE_THRESHOLD = 5.0\n",
    "\n",
    "np.random.seed(RANDOM_STATE)\n",
    "ds_logging.set_verbosity_error()\n",
    "\n",
    "# ---- 1) Load & rebuild abstracts ----\n",
    "ds = load_dataset(\"armanc/pubmed-rct20k\")\n",
    "train_df = ds[\"train\"].to_pandas()\n",
    "\n",
    "abstracts = (\n",
    "    train_df.sort_values([\"abstract_id\", \"sentence_id\"])\n",
    "            .groupby(\"abstract_id\")[\"text\"]\n",
    "            .apply(lambda s: \" \".join(s))\n",
    "            .reset_index()\n",
    ")\n",
    "\n",
    "# sample ~600 abstracts\n",
    "abstracts = abstracts.sample(n=SAMPLE_N, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "# ---- 2) Stoplists & markers (APPLIED AFTER LEMMA) ----\n",
    "# PubMed/trial boilerplate (keep domain signal like hiv/risk/hazard/survival)\n",
    "PUBMED_STOP = {\n",
    "    \"study\",\"studies\",\"background\",\"objective\",\"objectives\",\"method\",\"methods\",\n",
    "    \"result\",\"results\",\"conclusion\",\"conclusions\",\"introduction\",\"aim\",\"aims\",\n",
    "    \"patient\",\"patients\",\"participant\",\"participants\",\"subject\",\"subjects\",\n",
    "    \"group\",\"groups\",\"arm\",\"arms\",\"trial\",\"cohort\",\"sample\",\"samples\",\"baseline\",\n",
    "    \"randomize\",\"randomised\",\"randomized\",\"double\",\"blind\",\"double-blind\",\"placebo\",\n",
    "    \"include\",\"including\",\"perform\",\"performed\",\"report\",\"reported\",\"assess\",\"assessed\",\n",
    "    \"measure\",\"measured\",\"analyze\",\"analysis\",\"analyses\",\"associate\",\"associated\",\n",
    "    \"increase\",\"increased\",\"decrease\",\"decreased\",\"rate\",\"rates\",\n",
    "    \"week\",\"weeks\",\"month\",\"months\",\"year\",\"years\",\"daily\",\"usual\",\n",
    "    \"based\",\"level\",\"levels\",\"score\",\"scores\",\"change\",\"changes\",\"clinical\",\"life\",\n",
    "    \"respectively\",\"data\",\"follow\",\"follow-up\",\"difference\",\"differences\",\"receive\",\"received\",\n",
    "    \"effect\",\"effects\",\"primary\",\"event\",\"events\",\"quality\",\"safety\",\"reduction\",\"phase\"\n",
    "}\n",
    "\n",
    "# Generic glue (lemma forms). We target bare verbs/nouns you flagged.\n",
    "GLUE_STOP = {\n",
    "    \"this\",\"that\",\"there\",\"from\",\"over\",\"either\",\"only\",\"but\",\"most\",\"into\",\"under\",\n",
    "    \"who\",\"have\",\"had\",\"do\",\"did\",\"they\",\"those\",\"among\",\"also\",\"when\",\"per\",\n",
    "    \"than\",\"more\",\"both\",\"each\",\"all\",\"will\",\"be\",\"are\",\"is\",\"been\",\"after\",\"before\",\n",
    "    \"during\",\"between\",\"time\",\"day\",\"days\",\"mean\",\"range\",\"two\"\n",
    "}\n",
    "\n",
    "# Domain-generic process verbs/nouns (lemma) you donâ€™t want as anchors\n",
    "DOMAIN_GLUE = {\n",
    "    \"compare\",\"control\",\"significant\",\"significantly\",\"test\",\"find\",\"program\",\n",
    "    \"effective\",\"effectiveness\",\"intervention\",\"interventions\",\"outcome\",\"outcomes\",\n",
    "    \"assign\",\"randomly\",\"evaluate\",\"assess\",\"measure\",\"analyze\",\"detect\",\"improve\",\"improvement\"\n",
    "}\n",
    "\n",
    "# Dataset bracket/marker junk\n",
    "MARKER_TOKENS = {\n",
    "    \"-lsb-\",\"-rsb-\",\"-lrb-\",\"-rrb-\",\"-lsqb-\",\"-rsqb-\",\"lsb\",\"rsb\",\"lrb\",\"rrb\",\"lsqb\",\"rsqb\"\n",
    "}\n",
    "\n",
    "# tokens to drop if they contain digits or start with a hyphen (e.g., -mm, -week)\n",
    "def _bad_token(tok: str) -> bool:\n",
    "    return any(ch.isdigit() for ch in tok) or tok.startswith(\"-\")\n",
    "\n",
    "# ---- 3) Tokenization + Lemmatization (scispaCy) with 2-step stopwording ----\n",
    "# allow letters, hyphen, underscore (underscore marks phrases)\n",
    "_WORD_RE = re.compile(r\"[a-z\\-_]+\")\n",
    "\n",
    "def _simple_tokenize(text: str, base_stop):\n",
    "    text = text.lower()\n",
    "    toks = _WORD_RE.findall(text)\n",
    "    toks = [t for t in toks if len(t) > 2 and t not in base_stop and not _bad_token(t)]\n",
    "    return toks\n",
    "\n",
    "def tokenize_docs(texts):\n",
    "    \"\"\"\n",
    "    Layer 0: remove markers/digits/hyphen-leading crud\n",
    "    Layer 1: scispaCy lemma + NLTK English stopwords\n",
    "    Layer 2: PubMed boilerplate + glue stops (on LEMMAS)\n",
    "    \"\"\"\n",
    "    base_stop = set(MARKER_TOKENS)\n",
    "\n",
    "    try:\n",
    "        import spacy\n",
    "        from scispacy.abbreviation import AbbreviationDetector\n",
    "        from nltk.corpus import stopwords as _sw\n",
    "        import nltk; nltk.download(\"stopwords\", quiet=True)\n",
    "        en_sw = set(_sw.words(\"english\"))\n",
    "\n",
    "        nlp = spacy.load(\"en_core_sci_md\", disable=[\"ner\",\"textcat\"])\n",
    "        nlp.add_pipe(\"abbreviation_detector\")\n",
    "\n",
    "        out = []\n",
    "        for doc in nlp.pipe(texts, batch_size=64):\n",
    "            # Expand abbreviations (e.g., PFS -> progression free survival)\n",
    "            abbr_map = {str(abbr): str(abbr._.long_form) for abbr in doc._.abbreviations}\n",
    "            txt = doc.text\n",
    "            for abbr, longf in abbr_map.items():\n",
    "                txt = re.sub(rf\"\\b{re.escape(abbr)}\\b\", longf, txt, flags=re.IGNORECASE)\n",
    "\n",
    "            d2 = nlp.make_doc(txt)\n",
    "\n",
    "            # Layer 1: lemma + NLTK stopwords + base filters\n",
    "            toks_lemma = []\n",
    "            for t in d2:\n",
    "                s = (t.lemma_ or t.text).lower()\n",
    "                if not _WORD_RE.fullmatch(s):\n",
    "                    continue\n",
    "                if len(s) < 3:\n",
    "                    continue\n",
    "                if s in en_sw:\n",
    "                    continue\n",
    "                if s in base_stop or _bad_token(s):\n",
    "                    continue\n",
    "                toks_lemma.append(s)\n",
    "\n",
    "            # Layer 2: PubMed boilerplate + glue + domain-generic process words\n",
    "            toks = [t for t in toks_lemma if t not in PUBMED_STOP and t not in GLUE_STOP and t not in DOMAIN_GLUE]\n",
    "            out.append(toks)\n",
    "        return out\n",
    "\n",
    "    except Exception:\n",
    "        # Fallback: simple + NLTK layer + domain stops\n",
    "        try:\n",
    "            from nltk.corpus import stopwords as _sw\n",
    "            import nltk; nltk.download(\"stopwords\", quiet=True)\n",
    "            en_sw = set(_sw.words(\"english\"))\n",
    "        except Exception:\n",
    "            en_sw = set()\n",
    "\n",
    "        toks0 = [_simple_tokenize(t, base_stop) for t in texts]\n",
    "        toks1 = [[w for w in tl if w not in en_sw] for tl in toks0]\n",
    "        toks2 = [[w for w in tl if w not in PUBMED_STOP and w not in GLUE_STOP and w not in DOMAIN_GLUE] for tl in toks1]\n",
    "        return toks2\n",
    "\n",
    "tokenized = tokenize_docs(abstracts[\"text\"].tolist())\n",
    "tokenized = [tl for tl in tokenized if len(tl) >= MIN_TOKENS_DOC]\n",
    "\n",
    "# ---- 4) Phrase mining (bi -> tri), tuned for ~600 docs ----\n",
    "try:\n",
    "    from gensim.models.phrases import Phrases, Phraser\n",
    "    big = Phrases(tokenized, min_count=PHRASE_MIN_COUNT, threshold=PHRASE_THRESHOLD, delimiter=b\"_\")\n",
    "    big_p = Phraser(big)\n",
    "    tokenized = [big_p(tl) for tl in tokenized]\n",
    "\n",
    "    tri = Phrases(tokenized, min_count=PHRASE_MIN_COUNT, threshold=PHRASE_THRESHOLD, delimiter=b\"_\")\n",
    "    tri_p = Phraser(tri)\n",
    "    tokenized = [tri_p(tl) for tl in tokenized]\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# optional: hard-seed a few must-have phrases that often help interpretability\n",
    "MUST_PHRASES = [\n",
    "    (\"blood\", \"pressure\"), (\"weight\", \"loss\"),\n",
    "    (\"hazard\", \"ratio\"), (\"odds\", \"ratio\"),\n",
    "    (\"overall\", \"survival\"), (\"progression-free\", \"survival\"),\n",
    "    (\"adverse\", \"events\"), (\"ejection\", \"fraction\"),\n",
    "    (\"knee\", \"pain\"), (\"viral\", \"load\"), (\"cd4\", \"count\"), (\"smoking\", \"cessation\")\n",
    "]\n",
    "# join where present\n",
    "for tl in tokenized:\n",
    "    j = 0\n",
    "    while j < len(tl) - 1:\n",
    "        if (tl[j], tl[j+1]) in MUST_PHRASES:\n",
    "            tl[j:j+2] = [tl[j] + \"_\" + tl[j+1]]\n",
    "        else:\n",
    "            j += 1\n",
    "\n",
    "# ---- 5) Document-frequency filtering (no top-N cap) ----\n",
    "D_pre = len(tokenized)\n",
    "df = Counter()\n",
    "for tl in tokenized:\n",
    "    df.update(set(tl))\n",
    "\n",
    "min_df = max(MIN_DF_ABS, 1)\n",
    "max_df = int(MAX_DF_FRAC * D_pre)\n",
    "keep = {w for w, c in df.items() if (c >= min_df and c <= max_df)}\n",
    "\n",
    "tokenized = [[w for w in tl if w in keep] for tl in tokenized]\n",
    "\n",
    "# rebuild vocab (stable order)\n",
    "vocab_list = sorted(keep)\n",
    "vocab = {w: i for i, w in enumerate(vocab_list)}\n",
    "i2word = {i: w for w, i in vocab.items()}\n",
    "\n",
    "print(f\"Docs after cleaning: {len(tokenized)} | Vocab size: {len(vocab)} | min_df={min_df}, max_df={max_df}\")\n",
    "\n",
    "# ---- 6) Build words/docs for your Gibbs (names unchanged) ----\n",
    "words, docs = [], []\n",
    "for doc_id, doc in enumerate(tokenized):\n",
    "    for w in doc:\n",
    "        if w in vocab:\n",
    "            words.append(vocab[w])\n",
    "            docs.append(doc_id)\n",
    "\n",
    "words = np.array(words, dtype=int)\n",
    "docs  = np.array(docs,  dtype=int)\n",
    "\n",
    "D = int(docs.max()) + 1 if docs.size else 0\n",
    "W = int(words.max()) + 1 if words.size else 0\n",
    "N = len(docs)\n",
    "\n",
    "print(f\"#docs={D}, vocab size={W}, total tokens={N}\")\n",
    "\n",
    "# (Optional quick sanity)\n",
    "for i in range(min(2, len(tokenized))):\n",
    "    print(\"Sample tokens:\", tokenized[i][:25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47163902-367b-4afb-943d-82b5cc368ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our latent assignments Z and set up per-X-topic arrays to keep track of assignment totals\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# T = 5        # Only two topics (\"HCI\" and \"Graph theory\")\n",
    "# beta = .01     # LDA parameters: control sparsity of documents & topics\n",
    "# alpha = .001\n",
    "\n",
    "# T = 20\n",
    "# beta = 0.08\n",
    "# alpha = 0.06\n",
    "\n",
    "T = 18\n",
    "beta = 0.07\n",
    "alpha = 0.1\n",
    "\n",
    "a = np.zeros((D,T),dtype=int)   # total times a token in document d has been assigned to topic t\n",
    "b = np.zeros((W,T),dtype=int)   # total times word w has been assigned to topic t\n",
    "c = np.zeros((T,),dtype=int)    # total assignments to topic t\n",
    "\n",
    "z = np.random.randint(T,size=N);  # assign every token to a topic at random\n",
    "for n in range(N):\n",
    "    a[docs[n],z[n]] += 1             # and each document ID\n",
    "    b[words[n],z[n]] += 1             # count up number for each word ID\n",
    "    c[z[n]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c4a60ef-f9f8-48e5-ac3f-26ce53d3a502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_collapsed(maxIter):\n",
    "    for it in range(maxIter):             # for each iteration \n",
    "        if it%10 ==0 : print(it)\n",
    "        for i in range(N):                # run through all the words & sample z[i]:\n",
    "            t = z[i];\n",
    "            a[docs[i],t] -= 1                # remove token i's assignment from our count vectors\n",
    "            b[words[i],t] -= 1            \n",
    "            c[t]      -= 1\n",
    "\n",
    "            # Compute topic probability distribution given current counts\n",
    "            probs = (beta + b[words[i],:])/(c[:] + beta*W) * (alpha + a[docs[i],:])\n",
    "\n",
    "            # Now, normalize and draw a sample from the distribution over topics:\n",
    "            cumprobs = np.cumsum(probs); cumprobs /= cumprobs[-1] \n",
    "            t = np.where(cumprobs>np.random.rand())[0][0]\n",
    "\n",
    "            z[i] = t;                      # now add this assignment back into our count vectors\n",
    "            a[docs[i],t] += 1\n",
    "            b[words[i],t] += 1\n",
    "            c[t]      += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "db0a69ca-0497-4e88-b724-51be45b242aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for visualizing the learning process\n",
    "def print_topics():\n",
    "    '''Print the top 8 words in each topic given the current assignments'''\n",
    "    for t in range(T):\n",
    "        isort = np.argsort(-b[:,t])  # find the most likely words for topic t\n",
    "        xsort = b[isort,t]           # then print topic, % tokens explained, & top 8 words\n",
    "        print('[{}] ({:.3f}) {}'.format(t, 1.*c[t]/N, list(i2word[isort[ww]] for ww in range(8) if xsort[ww]>0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "acaaff47-00e8-489a-bbc7-a58efd69d8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] (0.054) ['treatment', 'compared', 'care', 'total', 'using', 'therapy', 'health', 'use']\n",
      "[1] (0.054) ['treatment', 'compared', 'risk', 'one', 'therapy', 'use', 'women', 'men']\n",
      "[2] (0.054) ['treatment', 'compared', 'using', 'efficacy', 'risk', 'controlled', 'pain', 'therapy']\n",
      "[3] (0.057) ['treatment', 'risk', 'care', 'women', 'compared', 'health', 'therapy', 'using']\n",
      "[4] (0.056) ['treatment', 'care', 'compared', 'health', 'pain', 'therapy', 'women', 'included']\n",
      "[5] (0.058) ['treatment', 'compared', 'care', 'risk', 'one', 'total', 'use', 'using']\n",
      "[6] (0.056) ['treatment', 'compared', 'therapy', 'total', 'care', 'surgery', 'using', 'risk']\n",
      "[7] (0.055) ['treatment', 'compared', 'women', 'adverse', 'lower', 'efficacy', 'children', 'surgery']\n",
      "[8] (0.054) ['treatment', 'compared', 'care', 'treated', 'period', 'lower', 'greater', 'efficacy']\n",
      "[9] (0.054) ['treatment', 'compared', 'total', 'pain', 'care', 'therapy', 'risk', 'using']\n",
      "[10] (0.055) ['treatment', 'compared', 'risk', 'one', 'women', 'health', 'pain', 'care']\n",
      "[11] (0.058) ['treatment', 'total', 'health', 'compared', 'versus', 'therapy', 'using', 'dose']\n",
      "[12] (0.057) ['treatment', 'compared', 'health', 'efficacy', 'use', 'pain', 'therapy', 'median']\n",
      "[13] (0.055) ['treatment', 'therapy', 'compared', 'similar', 'efficacy', 'care', 'one', 'observed']\n",
      "[14] (0.055) ['treatment', 'compared', 'efficacy', 'may', 'care', 'pain', 'risk', 'therapy']\n",
      "[15] (0.055) ['treatment', 'compared', 'age', 'using', 'therapy', 'efficacy', 'women', 'children']\n",
      "[16] (0.055) ['treatment', 'compared', 'care', 'using', 'adverse', 'higher', 'disease', 'therapy']\n",
      "[17] (0.055) ['treatment', 'compared', 'women', 'use', 'age', 'risk', 'using', 'care']\n"
     ]
    }
   ],
   "source": [
    "print_topics();       # before running any iterations: random assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "35506c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n",
      "960\n",
      "970\n",
      "980\n",
      "990\n",
      "1000\n",
      "1010\n",
      "1020\n",
      "1030\n",
      "1040\n",
      "1050\n",
      "1060\n",
      "1070\n",
      "1080\n",
      "1090\n",
      "1100\n",
      "1110\n",
      "1120\n",
      "1130\n",
      "1140\n",
      "1150\n",
      "1160\n",
      "1170\n",
      "1180\n",
      "1190\n",
      "1200\n",
      "1210\n",
      "1220\n",
      "1230\n",
      "1240\n",
      "1250\n",
      "1260\n",
      "1270\n",
      "1280\n",
      "1290\n",
      "1300\n",
      "1310\n",
      "1320\n",
      "1330\n",
      "1340\n",
      "1350\n",
      "1360\n",
      "1370\n",
      "1380\n",
      "1390\n",
      "1400\n",
      "1410\n",
      "1420\n",
      "1430\n",
      "1440\n",
      "1450\n",
      "1460\n",
      "1470\n",
      "1480\n",
      "1490\n",
      "1500\n",
      "1510\n",
      "1520\n",
      "1530\n",
      "1540\n",
      "1550\n",
      "1560\n",
      "1570\n",
      "1580\n",
      "1590\n",
      "1600\n",
      "1610\n",
      "1620\n",
      "1630\n",
      "1640\n",
      "1650\n",
      "1660\n",
      "1670\n",
      "1680\n",
      "1690\n",
      "1700\n",
      "1710\n",
      "1720\n",
      "1730\n",
      "1740\n",
      "1750\n",
      "1760\n",
      "1770\n",
      "1780\n",
      "1790\n",
      "1800\n",
      "1810\n",
      "1820\n",
      "1830\n",
      "1840\n",
      "1850\n",
      "1860\n",
      "1870\n",
      "1880\n",
      "1890\n",
      "1900\n",
      "1910\n",
      "1920\n",
      "1930\n",
      "1940\n",
      "1950\n",
      "1960\n",
      "1970\n",
      "1980\n",
      "1990\n",
      "2000\n",
      "2010\n",
      "2020\n",
      "2030\n",
      "2040\n",
      "2050\n",
      "2060\n",
      "2070\n",
      "2080\n",
      "2090\n",
      "2100\n",
      "2110\n",
      "2120\n",
      "2130\n",
      "2140\n",
      "2150\n",
      "2160\n",
      "2170\n",
      "2180\n",
      "2190\n",
      "2200\n",
      "2210\n",
      "2220\n",
      "2230\n",
      "2240\n",
      "2250\n",
      "2260\n",
      "2270\n",
      "2280\n",
      "2290\n",
      "2300\n",
      "2310\n",
      "2320\n",
      "2330\n",
      "2340\n",
      "2350\n",
      "2360\n",
      "2370\n",
      "2380\n",
      "2390\n",
      "2400\n",
      "2410\n",
      "2420\n",
      "2430\n",
      "2440\n",
      "2450\n",
      "2460\n",
      "2470\n",
      "2480\n",
      "2490\n",
      "2500\n",
      "2510\n",
      "2520\n",
      "2530\n",
      "2540\n",
      "2550\n",
      "2560\n",
      "2570\n",
      "2580\n",
      "2590\n",
      "[0] (0.065) ['pain', 'surgery', 'postoperative', 'total', 'incidence', 'complications', 'hospital', 'used']\n",
      "[1] (0.060) ['risk', 'stroke', 'disease', 'heart', 'renal', 'kidney', 'acute', 'coronary']\n",
      "[2] (0.056) ['risk', 'women', 'men', 'use', 'age', 'factors', 'hiv', 'health']\n",
      "[3] (0.050) ['cancer', 'women', 'chemotherapy', 'breast', 'screening', 'versus', 'response', 'median']\n",
      "[4] (0.088) ['care', 'health', 'controlled', 'management', 'using', 'adherence', 'costs', 'older']\n",
      "[5] (0.045) ['serum', 'concentrations', 'women', 'compared', 'vitamin', 'higher', 'total', 'standard']\n",
      "[6] (0.066) ['depression', 'cognitive', 'mental', 'symptoms', 'condition', 'depressive', 'whether', 'however']\n",
      "[7] (0.043) ['device', 'percentage', 'pressure', 'higher', 'target', 'block', 'nerve', 'compared']\n",
      "[8] (0.039) ['children', 'parents', 'infants', 'child', 'weight', 'eating', 'healthy', 'nutrition']\n",
      "[9] (0.047) ['mri', 'assessment', 'colonoscopy', 'imaging', 'cases', 'volume', 'lesions', 'using']\n",
      "[10] (0.053) ['exercise', 'training', 'physical', 'activity', 'education', 'students', 'using', 'higher']\n",
      "[11] (0.037) ['smoking', 'combination', 'response', 'therapy', 'plus', 'treatment', 'smokers', 'alone']\n",
      "[12] (0.055) ['treatment', 'therapy', 'syndrome', 'symptoms', 'efficacy', 'compared', 'total', 'function']\n",
      "[13] (0.090) ['treatment', 'dose', 'adverse', 'disease', 'children', 'doses', 'similar', 'response']\n",
      "[14] (0.056) ['glucose', 'acid', 'diabetes', 'insulin', 'plasma', 'diet', 'cholesterol', 'blood']\n",
      "[15] (0.063) ['treatment', 'versus', 'efficacy', 'alcohol', 'greater', 'ptsd', 'use', 'receiving']\n",
      "[16] (0.040) ['surgery', 'early', 'visual', 'different', 'eyes', 'age', 'statistically', 'motor']\n",
      "[17] (0.045) ['blood_pressure', 'min', 'compared', 'systolic', 'healthy', 'dose', 'diastolic', 'mmhg']\n"
     ]
    }
   ],
   "source": [
    "gibbs_collapsed(2600)            # run a bunch of iterations\n",
    "print_topics();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e7f735f-3607-4b3a-84ed-07197f544057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n",
      "960\n",
      "970\n",
      "980\n",
      "990\n",
      "1000\n",
      "1010\n",
      "1020\n",
      "1030\n",
      "1040\n",
      "1050\n",
      "1060\n",
      "1070\n",
      "1080\n",
      "1090\n",
      "1100\n",
      "1110\n",
      "1120\n",
      "1130\n",
      "1140\n",
      "1150\n",
      "1160\n",
      "1170\n",
      "1180\n",
      "1190\n",
      "1200\n",
      "1210\n",
      "1220\n",
      "1230\n",
      "1240\n",
      "1250\n",
      "1260\n",
      "1270\n",
      "1280\n",
      "1290\n",
      "1300\n",
      "1310\n",
      "1320\n",
      "1330\n",
      "1340\n",
      "1350\n",
      "1360\n",
      "1370\n",
      "1380\n",
      "1390\n",
      "1400\n",
      "1410\n",
      "1420\n",
      "1430\n",
      "1440\n",
      "1450\n",
      "1460\n",
      "1470\n",
      "1480\n",
      "1490\n",
      "1500\n",
      "1510\n",
      "1520\n",
      "1530\n",
      "1540\n",
      "1550\n",
      "1560\n",
      "1570\n",
      "1580\n",
      "1590\n",
      "1600\n",
      "1610\n",
      "1620\n",
      "1630\n",
      "1640\n",
      "1650\n",
      "1660\n",
      "1670\n",
      "1680\n",
      "1690\n",
      "1700\n",
      "1710\n",
      "1720\n",
      "1730\n",
      "1740\n",
      "1750\n",
      "1760\n",
      "1770\n",
      "1780\n",
      "1790\n",
      "1800\n",
      "1810\n",
      "1820\n",
      "1830\n",
      "1840\n",
      "1850\n",
      "1860\n",
      "1870\n",
      "1880\n",
      "1890\n",
      "1900\n",
      "1910\n",
      "1920\n",
      "1930\n",
      "1940\n",
      "1950\n",
      "1960\n",
      "1970\n",
      "1980\n",
      "1990\n",
      "2000\n",
      "2010\n",
      "2020\n",
      "2030\n",
      "2040\n",
      "2050\n",
      "2060\n",
      "2070\n",
      "2080\n",
      "2090\n",
      "2100\n",
      "2110\n",
      "2120\n",
      "2130\n",
      "2140\n",
      "2150\n",
      "2160\n",
      "2170\n",
      "2180\n",
      "2190\n",
      "2200\n",
      "2210\n",
      "2220\n",
      "2230\n",
      "2240\n",
      "2250\n",
      "2260\n",
      "2270\n",
      "2280\n",
      "2290\n",
      "2300\n",
      "2310\n",
      "2320\n",
      "2330\n",
      "2340\n",
      "2350\n",
      "2360\n",
      "2370\n",
      "2380\n",
      "2390\n",
      "2400\n",
      "2410\n",
      "2420\n",
      "2430\n",
      "2440\n",
      "2450\n",
      "2460\n",
      "2470\n",
      "2480\n",
      "2490\n",
      "[0] (0.028) ['smoking', 'alcohol', 'smokers', 'loss', 'abstinence', 'assessment', 'negative', 'drinking']\n",
      "[1] (0.049) ['significantly', 'statistically', 'compared', 'higher', 'nerve', 'found', 'test', 'success']\n",
      "[2] (0.026) ['asthma', 'versus', 'copd', 'cell', 'persistent', 'airway', 'fev', 'central']\n",
      "[3] (0.034) ['serum', 'kidney', 'renal', 'urinary', 'injury', 'min', 'compared', 'creatinine']\n",
      "[4] (0.053) ['risk', 'mortality', 'cardiovascular', 'outcome', 'disease', 'stroke', 'outcomes', 'coronary']\n",
      "[5] (0.032) ['cancer', 'women', 'screening', 'breast', 'colonoscopy', 'detection', 'compared', 'detected']\n",
      "[6] (0.069) ['care', 'intervention', 'patient', 'health', 'outcomes', 'management', 'effectiveness', 'controlled']\n",
      "[7] (0.045) ['children', 'infants', 'control', 'time', 'vaccine', 'range', 'oxygen', 'age']\n",
      "[8] (0.041) ['glucose', 'plasma', 'acid', 'insulin', 'diabetes', 'lower', 'cholesterol', 'blood']\n",
      "[9] (0.052) ['dose', 'serum', 'day', 'concentrations', 'healthy', 'doses', 'inflammation', 'infusion']\n",
      "[10] (0.046) ['exercise', 'control', 'training', 'performance', 'students', 'experimental', 'test', 'cardiac']\n",
      "[11] (0.057) ['symptoms', 'depression', 'treatment', 'cognitive', 'improvement', 'symptom', 'ptsd', 'improved']\n",
      "[12] (0.072) ['intervention', 'health', 'physical', 'activity', 'control', 'interventions', 'program', 'effective']\n",
      "[13] (0.034) ['mri', 'ventricular', 'left', 'imaging', 'ejection_fraction', 'radiation', 'lesions', 'dose']\n",
      "[14] (0.046) ['age', 'women', 'risk', 'men', 'hiv', 'health', 'factors', 'status']\n",
      "[15] (0.040) ['survival', 'cancer', 'chemotherapy', 'versus', 'median', 'response', 'grade', 'advanced']\n",
      "[16] (0.038) ['women', 'diet', 'intake', 'weight', 'consumption', 'healthy', 'dietary', 'control']\n",
      "[17] (0.074) ['pain', 'surgery', 'postoperative', 'control', 'visual', 'two', 'time', 'mean']\n",
      "[18] (0.113) ['treatment', 'adverse', 'efficacy', 'therapy', 'days', 'significantly', 'mean', 'combination']\n",
      "[19] (0.050) ['treatment', 'control', 'compared', 'two', 'blood_pressure', 'therapy', 'syndrome', 'efficacy']\n"
     ]
    }
   ],
   "source": [
    "gibbs_collapsed(2500)            # run a bunch of iterations\n",
    "print_topics();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccae148-a6f9-461e-9233-f7cd92652209",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
